#!/usr/bin/env python3
"""
LeRobotæ•°æ®é›†ç®¡ç†æ¨¡å—
ç”¨äºåˆ›å»ºã€ä¿å­˜å’Œç®¡ç†LeRobotæ ¼å¼çš„æ•°æ®é›†
"""

import os
import json
import numpy as np
import time
from pathlib import Path
import shutil
from datetime import datetime
from typing import Dict, Optional, Any
from rich.console import Console
from rich.logging import RichHandler
import logging

# å¯¼å…¥lerobotç›¸å…³æ¨¡å—
try:
    from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥lerobotæ¨¡å—ï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…lerobot")
    LeRobotDataset = None

# é…ç½®richæ—¥å¿—
console = Console()
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    datefmt="[%X]",
    handlers=[RichHandler(console=console, rich_tracebacks=True)]
)
logger = logging.getLogger("LeRobotDatasetManager")

class LeRobotDatasetManager:
    """LeRobotæ•°æ®é›†ç®¡ç†å™¨"""
    
    def __init__(self, 
                 dataset_dir: str = "piper_dataset",
                 repo_id: str = "piper/real-world-manipulation",
                 fps: float = 30.0,
                 task_description: str = "Real robot manipulation with Piper arm"):
        """
        åˆå§‹åŒ–æ•°æ®é›†ç®¡ç†å™¨
        
        Args:
            dataset_dir: æ•°æ®é›†ä¿å­˜ç›®å½•
            repo_id: æ•°æ®é›†ä»“åº“ID
            fps: å¸§ç‡
            task_description: ä»»åŠ¡æè¿°
        """
        self.dataset_dir = Path(dataset_dir).expanduser()
        self.repo_id = repo_id
        self.fps = fps
        self.task_description = task_description
        
        self.dataset = None
        self.episode_idx = 0
        self.frame_count = 0
        self.is_recording = False
        
        # æ•°æ®é›†ç‰¹å¾å®šä¹‰
        self.features = {
            # æ‘„åƒå¤´å›¾åƒ (CHWæ ¼å¼)
            "observation.images.ee_cam": {"shape": (3, 480, 640), "dtype": "image"},
            "observation.images.rgb_rs_0": {"shape": (3, 480, 640), "dtype": "image"},
            "observation.images.rgb_rs_1": {"shape": (3, 480, 640), "dtype": "image"},
            # å…³èŠ‚çŠ¶æ€ (åªæœ‰å…³èŠ‚ä½ç½®8ç»´ï¼š6ä¸ªä¸»å…³èŠ‚ + 2ä¸ªå¤¹çˆª)
            "observation.state": {"shape": (8,), "dtype": "float32"},
            # åŠ¨ä½œ (8ç»´å…³èŠ‚ç»å¯¹ä½ç½®æ§åˆ¶)
            "actions": {"shape": (8,), "dtype": "float32"}
        }
        
        logger.info("âœ… LeRobotæ•°æ®é›†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def create_dataset(self, resume: bool = False) -> bool:
        """
        åˆ›å»ºæˆ–åŠ è½½æ•°æ®é›†
        
        Args:
            resume: æ˜¯å¦ä»ç°æœ‰æ•°æ®é›†ç»§ç»­å½•åˆ¶
            
        Returns:
            åˆ›å»º/åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        if LeRobotDataset is None:
            logger.error("LeRobotDatasetæœªå¯¼å…¥ï¼Œæ— æ³•åˆ›å»ºæ•°æ®é›†")
            return False
        
        try:
            # å¤„ç†å¢é‡å½•åˆ¶é€»è¾‘
            if resume and self.dataset_dir.exists():
                existing_episodes = self._get_episode_count()
                logger.info(f"å¢é‡å½•åˆ¶æ¨¡å¼ï¼šæ£€æµ‹åˆ° {existing_episodes} ä¸ªå·²æœ‰episode")
                self.episode_idx = existing_episodes
                
                try:
                    # å°è¯•åŠ è½½ç°æœ‰æ•°æ®é›†
                    self.dataset = LeRobotDataset(
                        repo_id=self.repo_id,
                        root=self.dataset_dir
                    )
                    logger.info(f"âœ… æˆåŠŸåŠ è½½ç°æœ‰æ•°æ®é›†ï¼Œå°†ä»episode {self.episode_idx}å¼€å§‹å½•åˆ¶")
                    return True
                    
                except Exception as e:
                    logger.warning(f"åŠ è½½ç°æœ‰æ•°æ®é›†å¤±è´¥: {e}")
                    logger.info("å°†åˆ›å»ºæ–°æ•°æ®é›†...")
                    
                    # åˆ›å»ºå¤‡ä»½
                    if self.dataset_dir.exists():
                        backup_path = f"{self.dataset_dir}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                        logger.info(f"åˆ›å»ºå¤‡ä»½ï¼š{backup_path}")
                        shutil.copytree(self.dataset_dir, backup_path)
                        shutil.rmtree(self.dataset_dir)
                    
                    self.episode_idx = 0
            else:
                # éå¢é‡æ¨¡å¼ï¼Œåˆ é™¤å·²æœ‰ç›®å½•
                if self.dataset_dir.exists():
                    logger.info(f"åˆ é™¤å·²æœ‰æ•°æ®é›†ç›®å½•ï¼š{self.dataset_dir}")
                    shutil.rmtree(self.dataset_dir)
                self.episode_idx = 0
            
            # åˆ›å»ºæ–°æ•°æ®é›†
            self.dataset = LeRobotDataset.create(
                repo_id=self.repo_id,
                root=self.dataset_dir,
                fps=self.fps,
                features=self.features
            )
            
            # åˆ›å»ºmodalityæ–‡ä»¶
            self._create_modality_file()
            
            logger.info(f"âœ… æˆåŠŸåˆ›å»ºæ–°æ•°æ®é›†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ•°æ®é›†å¤±è´¥: {e}")
            return False
    
    def _get_episode_count(self) -> int:
        """è·å–æ•°æ®é›†ä¸­å·²æœ‰çš„episodeæ•°é‡"""
        try:
            if not self.dataset_dir.exists():
                return 0
            
            # æ–¹æ³•1: è¯»å–meta/info.json
            info_path = self.dataset_dir / "meta" / "info.json"
            if info_path.exists():
                with open(info_path, "r") as f:
                    info = json.load(f)
                    return info.get("total_episodes", 0)
            
            # æ–¹æ³•2: ç»Ÿè®¡episodeæ–‡ä»¶
            episode_count = 0
            data_path = self.dataset_dir / "data"
            if data_path.exists():
                for chunk_dir in data_path.glob("chunk-*"):
                    episode_files = list(chunk_dir.glob("episode_*.parquet"))
                    episode_count += len(episode_files)
            
            return episode_count
            
        except Exception as e:
            logger.warning(f"è·å–episodeæ•°é‡å¤±è´¥: {e}")
            return 0
    
    def _create_modality_file(self):
        """åˆ›å»ºmodality.jsonæ–‡ä»¶"""
        modality_path = self.dataset_dir / "meta" / "modality.json"
        if not modality_path.exists():
            modality_path.parent.mkdir(parents=True, exist_ok=True)
            modality_data = {
                "observation.state": {
                    "joint_position": {"start": 0, "end": 8}  # å…³èŠ‚ä½ç½® 8ç»´ï¼ˆ6ä¸ªä¸»å…³èŠ‚ + 2ä¸ªå¤¹çˆªï¼‰
                },
                "actions": {
                    "joint_target": {"start": 0, "end": 8, "absolute": True}  # å…³èŠ‚ç»å¯¹ä½ç½®ç›®æ ‡ 8ç»´
                }
            }
            
            with open(modality_path, "w") as f:
                json.dump(modality_data, f, indent=2)
            
            logger.info(f"åˆ›å»ºmodality.jsonæ–‡ä»¶ï¼š{modality_path}")
    
    def start_episode(self):
        """å¼€å§‹æ–°çš„episode"""
        if self.dataset is None:
            logger.error("æ•°æ®é›†æœªåˆå§‹åŒ–ï¼Œæ— æ³•å¼€å§‹episode")
            return False
        
        if self.is_recording:
            logger.warning("å·²ç»åœ¨å½•åˆ¶ä¸­ï¼Œè¯·å…ˆç»“æŸå½“å‰episode")
            return False
        
        self.is_recording = True
        self.frame_count = 0
        logger.info(f"å¼€å§‹å½•åˆ¶ episode {self.episode_idx}")
        return True
    
    def add_frame(self, 
                  camera_images: Dict[str, np.ndarray],
                  robot_state: np.ndarray,
                  actions: np.ndarray,
                  task: str = None) -> bool:
        """
        æ·»åŠ ä¸€å¸§æ•°æ®
        
        Args:
            camera_images: ç›¸æœºå›¾åƒå­—å…¸ï¼Œkeyä¸ºç›¸æœºåç§°ï¼Œvalueä¸ºå›¾åƒæ•°ç»„(CHWæ ¼å¼)
            robot_state: æœºæ¢°è‡‚çŠ¶æ€å‘é‡(8ç»´)
            actions: åŠ¨ä½œå‘é‡(8ç»´)
            task: ä»»åŠ¡æè¿°
            
        Returns:
            æ·»åŠ æ˜¯å¦æˆåŠŸ
        """
        if not self.is_recording or self.dataset is None:
            logger.error("æœªåœ¨å½•åˆ¶çŠ¶æ€æˆ–æ•°æ®é›†æœªåˆå§‹åŒ–")
            return False
        
        try:
            # éªŒè¯è¾“å…¥æ•°æ®çš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
            if not isinstance(camera_images, dict):
                logger.error("ç›¸æœºå›¾åƒæ•°æ®ç±»å‹é”™è¯¯ï¼Œåº”ä¸ºå­—å…¸")
                return False
                
            if robot_state is None or not isinstance(robot_state, np.ndarray) or len(robot_state) != 8:
                logger.error(f"æœºæ¢°è‡‚çŠ¶æ€æ•°æ®æ— æ•ˆ: type={type(robot_state)}, shape={getattr(robot_state, 'shape', 'N/A')}")
                return False
                
            if actions is None or not isinstance(actions, np.ndarray) or len(actions) != 8:
                logger.error(f"åŠ¨ä½œæ•°æ®æ— æ•ˆ: type={type(actions)}, shape={getattr(actions, 'shape', 'N/A')}")
                return False
            
            # æ„å»ºå¸§æ•°æ®
            frame = {}
            
            # ä¸¥æ ¼éªŒè¯å’Œæ·»åŠ ç›¸æœºå›¾åƒ
            required_cameras = ["observation.images.ee_cam", "observation.images.rgb_rs_0", "observation.images.rgb_rs_1"]
            for key in required_cameras:
                if key in camera_images and camera_images[key] is not None:
                    img = camera_images[key]
                    
                    # éªŒè¯å›¾åƒæ•°æ®ç±»å‹å’Œå°ºå¯¸
                    if not isinstance(img, np.ndarray):
                        logger.error(f"å›¾åƒ {key} æ•°æ®ç±»å‹é”™è¯¯: {type(img)}")
                        return False
                    
                    # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®(CHW, uint8)
                    if img.dtype != np.uint8:
                        img = (img * 255).astype(np.uint8) if img.max() <= 1.0 else img.astype(np.uint8)
                    
                    # éªŒè¯å›¾åƒå°ºå¯¸
                    if img.shape != (3, 480, 640):
                        logger.error(f"å›¾åƒ {key} å°ºå¯¸ä¸åŒ¹é…: æœŸæœ›(3,480,640), å®é™…{img.shape}")
                        return False
                    
                    frame[key] = img
                else:
                    logger.error(f"å¿…éœ€çš„ç›¸æœºå›¾åƒç¼ºå¤±: {key}")
                    return False
            
            # æ·»åŠ æœºæ¢°è‡‚çŠ¶æ€
            frame["observation.state"] = robot_state.astype(np.float32)
            
            # æ·»åŠ åŠ¨ä½œ
            frame["actions"] = actions.astype(np.float32)
            
            # æœ€ç»ˆéªŒè¯å¸§æ•°æ®å®Œæ•´æ€§
            expected_keys = ["observation.images.ee_cam", "observation.images.rgb_rs_0", 
                           "observation.images.rgb_rs_1", "observation.state", "actions"]
            for key in expected_keys:
                if key not in frame:
                    logger.error(f"å¸§æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {key}")
                    return False
                    
                # éªŒè¯æ•°æ®ä¸ä¸ºNone
                if frame[key] is None:
                    logger.error(f"å¸§æ•°æ®å­—æ®µä¸ºNone: {key}")
                    return False
            
            # æ·»åŠ åˆ°æ•°æ®é›†
            try:
                if task is not None:
                    self.dataset.add_frame(frame, task)
                else:
                    self.dataset.add_frame(frame, self.task_description)
                
                # åªæœ‰åœ¨æˆåŠŸæ·»åŠ åˆ°æ•°æ®é›†åæ‰å¢åŠ è®¡æ•°å™¨
                self.frame_count += 1
                
                # æ¯10å¸§è¾“å‡ºä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
                if self.frame_count % 10 == 0:
                    logger.info(f"å·²æ·»åŠ  {self.frame_count} å¸§åˆ°episode {self.episode_idx}")
                
                # éªŒè¯æ•°æ®é›†bufferçŠ¶æ€ï¼ˆæ¯å¸§éƒ½æ£€æŸ¥ï¼Œä½†åªåœ¨é—®é¢˜æ—¶è¾“å‡ºï¼‰
                if hasattr(self.dataset, 'episode_buffer'):
                    buffer = self.dataset.episode_buffer
                    if isinstance(buffer, dict):
                        # åªæ£€æŸ¥å®é™…çš„æ•°æ®å­—æ®µ
                        data_fields = [
                            'observation.images.ee_cam',
                            'observation.images.rgb_rs_0', 
                            'observation.images.rgb_rs_1',
                            'observation.state',
                            'actions',
                            'task',
                            'timestamp',
                            'frame_index'
                        ]
                        
                        for key in data_fields:
                            if key in buffer and hasattr(buffer[key], '__len__'):
                                if len(buffer[key]) != self.frame_count:
                                    logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ•°æ®é•¿åº¦ä¸ä¸€è‡´: {key}={len(buffer[key])}, frame_count={self.frame_count}")
                                    break
                
                return True
                
            except Exception as dataset_e:
                logger.error(f"æ·»åŠ å¸§åˆ°æ•°æ®é›†å¤±è´¥: {dataset_e}")
                logger.error(f"é”™è¯¯ç±»å‹: {type(dataset_e).__name__}")
                
                # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                logger.error(f"å¸§æ•°æ®è¯¦ç»†ä¿¡æ¯:")
                for key, value in frame.items():
                    if hasattr(value, 'shape'):
                        logger.error(f"  {key}: shape={value.shape}, dtype={value.dtype}")
                    else:
                        logger.error(f"  {key}: type={type(value)}, value={value}")
                
                return False
            
        except Exception as e:
            logger.error(f"æ„å»ºå¸§æ•°æ®å¤±è´¥: {e}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            return False
    
    def end_episode(self) -> bool:
        """ç»“æŸå½“å‰episode"""
        logger.info("ğŸ”¹" * 30)
        logger.info("ğŸ“ å¼€å§‹ç»“æŸepisodeæµç¨‹")
        
        if not self.is_recording or self.dataset is None:
            logger.error("âŒ æœªåœ¨å½•åˆ¶çŠ¶æ€æˆ–æ•°æ®é›†æœªåˆå§‹åŒ–")
            logger.error(f"  - is_recording: {self.is_recording}")
            logger.error(f"  - dataset: {self.dataset is not None}")
            return False
        
        logger.info(f"ğŸ“Š å½“å‰çŠ¶æ€:")
        logger.info(f"  - episode_idx: {self.episode_idx}")
        logger.info(f"  - frame_count: {self.frame_count}")
        logger.info(f"  - is_recording: {self.is_recording}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å¸§æ•°æ®
        if self.frame_count == 0:
            logger.warning("âš ï¸ å½“å‰episodeæ²¡æœ‰å½•åˆ¶ä»»ä½•å¸§æ•°æ®ï¼Œè·³è¿‡ä¿å­˜")
            # é‡ç½®çŠ¶æ€ä½†ä¸ä¿å­˜
            self.is_recording = False
            self.frame_count = 0
            logger.info("âœ… çŠ¶æ€å·²é‡ç½®")
            return True
        
        # æ”¾å®½æœ€å°å¸§æ•°è¦æ±‚ - ä»5å¸§é™ä½åˆ°2å¸§ï¼Œä½¿ä¿å­˜æ›´å®¹æ˜“æˆåŠŸ
        min_frames = 2  # é™ä½æœ€å°å¸§æ•°è¦æ±‚
        if self.frame_count < min_frames:
            logger.warning(f"âš ï¸ å½“å‰episodeå¸§æ•°å¤ªå°‘ ({self.frame_count} < {min_frames})ï¼Œè·³è¿‡ä¿å­˜")
            self.is_recording = False
            self.frame_count = 0
            logger.info("âœ… çŠ¶æ€å·²é‡ç½®")
            return True
        
        logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜episode {self.episode_idx}ï¼Œå…± {self.frame_count} å¸§...")
        
        try:
            # Step 1: åŒæ­¥å¸§è®¡æ•°
            logger.info("ğŸ”„ æ­¥éª¤1: åŒæ­¥å¸§è®¡æ•°...")
            synced_count = self._sync_frame_count()
            logger.info(f"   åŒæ­¥ç»“æœ: {self.frame_count} -> {synced_count}")
            
            # Step 2: æ£€æŸ¥bufferçŠ¶æ€
            logger.info("ğŸ” æ­¥éª¤2: æ£€æŸ¥episode buffer...")
            buffer_ok = self._check_buffer_status()
            if not buffer_ok:
                logger.error("âŒ BufferçŠ¶æ€æ£€æŸ¥å¤±è´¥")
                self._reset_episode_state()
                return False
            
            # Step 3: å°è¯•ä¿®å¤æ•°æ®ä¸ä¸€è‡´é—®é¢˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            logger.info("ğŸ”§ æ­¥éª¤3: å°è¯•ä¿®å¤æ•°æ®ä¸ä¸€è‡´...")
            fix_ok = self._try_fix_data_inconsistency(min_frames)
            if not fix_ok:
                logger.error("âŒ æ•°æ®ä¿®å¤å¤±è´¥")
                self._reset_episode_state()
                return False
            
            # Step 4: æœ€ç»ˆéªŒè¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œæ›´å®¹é”™ï¼‰
            logger.info("âœ… æ­¥éª¤4: æœ€ç»ˆæ•°æ®éªŒè¯...")
            if not self._validate_episode_data_simple():
                logger.error("âŒ æœ€ç»ˆæ•°æ®éªŒè¯å¤±è´¥")
                self._reset_episode_state()
                return False
            
            # Step 5: ä¿å­˜episode
            logger.info("ğŸ’¾ æ­¥éª¤5: ä¿å­˜episodeåˆ°ç£ç›˜...")
            save_start = time.time()
            self.dataset.save_episode()
            save_time = time.time() - save_start
            
            logger.info(f"âœ… Episode {self.episode_idx} ä¿å­˜æˆåŠŸ!")
            logger.info(f"   ğŸ“ˆ æ•°æ®ç»Ÿè®¡: {self.frame_count} å¸§")
            logger.info(f"   â±ï¸  ä¿å­˜è€—æ—¶: {save_time:.2f}ç§’")
            
            # æ›´æ–°çŠ¶æ€
            self.episode_idx += 1
            self.frame_count = 0
            self.is_recording = False
            
            logger.info("ğŸ”¹" * 30)
            return True
            
        except Exception as e:
            logger.error("âŒ" * 30)
            logger.error(f"ğŸ’¥ ä¿å­˜episodeå¼‚å¸¸: {e}")
            logger.error(f"ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"ğŸ“„ é”™è¯¯è¯¦æƒ…: {str(e)}")
            
            # æ‰“å°å…³é”®é”™è¯¯å †æ ˆï¼ˆç®€åŒ–ç‰ˆï¼‰
            import traceback
            error_lines = traceback.format_exc().split('\n')
            # åªæ˜¾ç¤ºæœ€åå‡ è¡Œå…³é”®é”™è¯¯ä¿¡æ¯
            key_lines = [line for line in error_lines if any(keyword in line.lower() for keyword in ['error', 'exception', 'failed', 'file', 'line'])]
            if key_lines:
                logger.error("ğŸ” å…³é”®é”™è¯¯ä¿¡æ¯:")
                for line in key_lines[-5:]:  # æœ€å5è¡Œå…³é”®ä¿¡æ¯
                    if line.strip():
                        logger.error(f"   {line.strip()}")
            
            # ç®€åŒ–çš„è°ƒè¯•ä¿¡æ¯
            self._log_simple_debug_info()
            
            # é‡ç½®çŠ¶æ€
            self._reset_episode_state()
            
            logger.error("âŒ" * 30)
            return False
    
    def _check_buffer_status(self) -> bool:
        """æ£€æŸ¥episode bufferçŠ¶æ€"""
        try:
            if not hasattr(self.dataset, 'episode_buffer'):
                logger.error("âŒ Datasetæ²¡æœ‰episode_bufferå±æ€§")
                return False
            
            buffer = self.dataset.episode_buffer
            if not isinstance(buffer, dict):
                logger.error(f"âŒ Episode bufferç±»å‹é”™è¯¯: {type(buffer)}")
                return False
            
            logger.info(f"âœ… BufferçŠ¶æ€æ­£å¸¸: {len(buffer)} ä¸ªå­—æ®µ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ BufferçŠ¶æ€æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    def _try_fix_data_inconsistency(self, min_frames: int) -> bool:
        """å°è¯•ä¿®å¤æ•°æ®ä¸ä¸€è‡´é—®é¢˜"""
        try:
            if not hasattr(self.dataset, 'episode_buffer'):
                return True  # å¦‚æœæ²¡æœ‰bufferï¼Œè·³è¿‡ä¿®å¤
            
            buffer = self.dataset.episode_buffer
            if not isinstance(buffer, dict):
                return True  # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œè·³è¿‡ä¿®å¤
            
            # æ£€æŸ¥å…³é”®æ•°æ®å­—æ®µçš„é•¿åº¦
            data_fields = [
                'observation.images.ee_cam',
                'observation.images.rgb_rs_0', 
                'observation.images.rgb_rs_1',
                'observation.state',
                'actions'
            ]
            
            lengths = {}
            for key in data_fields:
                if key in buffer and hasattr(buffer[key], '__len__'):
                    lengths[key] = len(buffer[key])
            
            if not lengths:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ•°æ®å­—æ®µï¼Œè·³è¿‡ä¿®å¤")
                return True
            
            # æ£€æŸ¥é•¿åº¦æ˜¯å¦ä¸€è‡´
            unique_lengths = set(lengths.values())
            if len(unique_lengths) == 1:
                # é•¿åº¦ä¸€è‡´ï¼Œä¸éœ€è¦ä¿®å¤
                actual_length = list(unique_lengths)[0]
                logger.info(f"âœ… æ•°æ®é•¿åº¦ä¸€è‡´: {actual_length}")
                
                # ç¡®ä¿sizeå­—æ®µæ­£ç¡®
                buffer['size'] = actual_length
                self.frame_count = actual_length
                return True
            
            # é•¿åº¦ä¸ä¸€è‡´ï¼Œå°è¯•ä¿®å¤
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ•°æ®é•¿åº¦ä¸ä¸€è‡´: {lengths}")
            
            # ä½¿ç”¨æœ€å°é•¿åº¦
            min_length = min(lengths.values())
            logger.info(f"ğŸ”§ ä½¿ç”¨æœ€å°é•¿åº¦ä¿®å¤: {min_length}")
            
            if min_length < min_frames:
                logger.error(f"âŒ æœ€å°é•¿åº¦å¤ªå° ({min_length} < {min_frames})")
                return False
            
            # æˆªå–æ‰€æœ‰å­—æ®µåˆ°æœ€å°é•¿åº¦
            all_fields = list(buffer.keys())
            for key in all_fields:
                if key in buffer and hasattr(buffer[key], '__len__') and hasattr(buffer[key], '__getitem__'):
                    if len(buffer[key]) > min_length:
                        buffer[key] = buffer[key][:min_length]
                        logger.info(f"   æˆªå– {key}: {len(buffer[key])} -> {min_length}")
            
            # æ›´æ–°çŠ¶æ€
            buffer['size'] = min_length
            self.frame_count = min_length
            
            logger.info(f"âœ… æ•°æ®ä¿®å¤å®Œæˆï¼Œç»Ÿä¸€é•¿åº¦: {min_length}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®ä¿®å¤å¼‚å¸¸: {e}")
            return False
    
    def _validate_episode_data_simple(self) -> bool:
        """ç®€åŒ–çš„episodeæ•°æ®éªŒè¯"""
        try:
            if not hasattr(self.dataset, 'episode_buffer'):
                logger.error("âŒ æ²¡æœ‰episode_buffer")
                return False
            
            buffer = self.dataset.episode_buffer
            if not isinstance(buffer, dict):
                logger.error("âŒ episode_bufferä¸æ˜¯å­—å…¸")
                return False
            
            # æ£€æŸ¥åŸºæœ¬å­—æ®µå­˜åœ¨
            required_keys = [
                'observation.images.ee_cam',
                'observation.images.rgb_rs_0', 
                'observation.images.rgb_rs_1',
                'observation.state',
                'actions'
            ]
            
            missing_keys = []
            for key in required_keys:
                if key not in buffer:
                    missing_keys.append(key)
            
            if missing_keys:
                logger.error(f"âŒ ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_keys}")
                return False
            
            # æ£€æŸ¥æ•°æ®ä¸ä¸ºç©º
            for key in required_keys:
                if not buffer[key] or len(buffer[key]) == 0:
                    logger.error(f"âŒ å­—æ®µ {key} ä¸ºç©º")
                    return False
            
            # ç¡®ä¿sizeå­—æ®µå­˜åœ¨ä¸”æ­£ç¡®
            if 'size' not in buffer:
                buffer['size'] = self.frame_count
            
            logger.info(f"âœ… æ•°æ®éªŒè¯é€šè¿‡: {self.frame_count} å¸§")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®éªŒè¯å¼‚å¸¸: {e}")
            return False
    
    def _log_simple_debug_info(self):
        """è®°å½•ç®€åŒ–çš„è°ƒè¯•ä¿¡æ¯"""
        try:
            logger.error("ğŸ” ç®€åŒ–è°ƒè¯•ä¿¡æ¯:")
            logger.error(f"  ğŸ“Š å¸§è®¡æ•°: {self.frame_count}")
            logger.error(f"  ğŸ“ Episodeç´¢å¼•: {self.episode_idx}")
            logger.error(f"  ğŸ¬ å½•åˆ¶çŠ¶æ€: {self.is_recording}")
            
            if hasattr(self.dataset, 'episode_buffer'):
                buffer = self.dataset.episode_buffer
                if isinstance(buffer, dict):
                    data_keys = ['observation.images.ee_cam', 'observation.state', 'actions']
                    for key in data_keys:
                        if key in buffer and hasattr(buffer[key], '__len__'):
                            logger.error(f"  ğŸ“‹ {key}: {len(buffer[key])} é¡¹")
                        else:
                            logger.error(f"  âŒ {key}: ç¼ºå¤±æˆ–æ— æ•ˆ")
                else:
                    logger.error(f"  âŒ Bufferç±»å‹é”™è¯¯: {type(buffer)}")
            else:
                logger.error("  âŒ æ²¡æœ‰episode_buffer")
                
        except Exception as e:
            logger.error(f"ç®€åŒ–è°ƒè¯•ä¿¡æ¯è·å–å¤±è´¥: {e}")
    
    def _log_debug_info(self):
        """è®°å½•è°ƒè¯•ä¿¡æ¯"""
        try:
            logger.error("=================== è¯¦ç»†è°ƒè¯•ä¿¡æ¯ ===================")
            
            if hasattr(self.dataset, 'episode_buffer'):
                buffer = self.dataset.episode_buffer
                logger.error(f"Episode bufferè¯¦ç»†ä¿¡æ¯:")
                logger.error(f"  - ç±»å‹: {type(buffer)}")
                logger.error(f"  - é•¿åº¦: {len(buffer) if hasattr(buffer, '__len__') else 'N/A'}")
                
                if isinstance(buffer, dict):
                    logger.error(f"  - Keys: {list(buffer.keys())}")
                    
                    # è¯¦ç»†åˆ†ææ¯ä¸ªå­—æ®µ
                    for key, value in buffer.items():
                        logger.error(f"  - {key}:")
                        if hasattr(value, 'shape'):
                            logger.error(f"      shape={value.shape}, dtype={value.dtype}")
                            if hasattr(value, '__len__'):
                                logger.error(f"      length={len(value)}")
                        elif hasattr(value, '__len__'):
                            logger.error(f"      length={len(value)}, type={type(value)}")
                            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œæ˜¾ç¤ºå‰å‡ ä¸ªå…ƒç´ çš„ä¿¡æ¯
                            if isinstance(value, list) and len(value) > 0:
                                first_elem = value[0]
                                logger.error(f"      first_element_type={type(first_elem)}")
                                if hasattr(first_elem, 'shape'):
                                    logger.error(f"      first_element_shape={first_elem.shape}")
                        else:
                            logger.error(f"      value={value}, type={type(value)}")
                    
                    # æ£€æŸ¥æ•°æ®é•¿åº¦ä¸€è‡´æ€§ - åªæ£€æŸ¥å®é™…çš„æ•°æ®å­—æ®µ
                    data_fields = [
                        'observation.images.ee_cam',
                        'observation.images.rgb_rs_0', 
                        'observation.images.rgb_rs_1',
                        'observation.state',
                        'actions',
                        'task',
                        'timestamp',
                        'frame_index'
                    ]
                    
                    lengths = {}
                    for key in data_fields:
                        if key in buffer and hasattr(buffer[key], '__len__'):
                            lengths[key] = len(buffer[key])
                    
                    if lengths:
                        logger.error(f"æ•°æ®é•¿åº¦ç»Ÿè®¡:")
                        for key, length in lengths.items():
                            logger.error(f"  {key}: {length}")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸ä¸€è‡´çš„é•¿åº¦
                        unique_lengths = set(lengths.values())
                        if len(unique_lengths) > 1:
                            logger.error(f"âš ï¸ æ£€æµ‹åˆ°æ•°æ®é•¿åº¦ä¸ä¸€è‡´!")
                            logger.error(f"ä¸åŒçš„é•¿åº¦å€¼: {sorted(unique_lengths)}")
                            
                            # æŒ‰é•¿åº¦åˆ†ç»„
                            length_groups = {}
                            for key, length in lengths.items():
                                if length not in length_groups:
                                    length_groups[length] = []
                                length_groups[length].append(key)
                            
                            for length, keys in length_groups.items():
                                logger.error(f"é•¿åº¦ {length}: {keys}")
                        else:
                            logger.error(f"âœ… æ‰€æœ‰æ•°æ®é•¿åº¦ä¸€è‡´: {list(unique_lengths)[0]}")
                    
                    # æ£€æŸ¥sizeå­—æ®µ
                    if 'size' in buffer:
                        logger.error(f"sizeå­—æ®µå€¼: {buffer['size']}")
                    else:
                        logger.error("âš ï¸ ç¼ºå°‘sizeå­—æ®µ")
            
            # å…¶ä»–ç›¸å…³çŠ¶æ€ä¿¡æ¯
            logger.error(f"æœ¬åœ°çŠ¶æ€ä¿¡æ¯:")
            logger.error(f"  - frame_count: {self.frame_count}")
            logger.error(f"  - episode_idx: {self.episode_idx}")
            logger.error(f"  - is_recording: {self.is_recording}")
            
            logger.error("=================== è°ƒè¯•ä¿¡æ¯ç»“æŸ ===================")
            
        except Exception as debug_e:
            logger.error(f"è°ƒè¯•ä¿¡æ¯è·å–å¤±è´¥: {debug_e}")
            import traceback
            logger.error(f"è°ƒè¯•ä¿¡æ¯è·å–é”™è¯¯å †æ ˆ:")
            logger.error(traceback.format_exc())
    
    def _reset_episode_state(self):
        """é‡ç½®episodeçŠ¶æ€"""
        try:
            if hasattr(self.dataset, 'reset_episode'):
                self.dataset.reset_episode()
                logger.info("å·²é‡ç½®dataset episodeçŠ¶æ€")
        except Exception as reset_e:
            logger.warning(f"é‡ç½®dataset episodeçŠ¶æ€å¤±è´¥: {reset_e}")
        
        # é‡ç½®æœ¬åœ°çŠ¶æ€
        self.is_recording = False
        self.frame_count = 0
        logger.info("å·²é‡ç½®æœ¬åœ°çŠ¶æ€")
    
    def get_dataset_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        info = {
            "dataset_dir": str(self.dataset_dir),
            "repo_id": self.repo_id,
            "fps": self.fps,
            "current_episode": self.episode_idx,
            "frame_count": self.frame_count,
            "is_recording": self.is_recording,
            "total_episodes": self._get_episode_count()
        }
        
        # å¦‚æœæ•°æ®é›†å­˜åœ¨ï¼Œæ·»åŠ æ›´å¤šä¿¡æ¯
        if self.dataset_dir.exists():
            info_file = self.dataset_dir / "meta" / "info.json"
            if info_file.exists():
                try:
                    with open(info_file, "r") as f:
                        meta_info = json.load(f)
                        info.update(meta_info)
                except Exception as e:
                    logger.warning(f"è¯»å–æ•°æ®é›†å…ƒä¿¡æ¯å¤±è´¥: {e}")
        
        return info
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.is_recording:
            logger.warning("å½•åˆ¶æœªå®Œæˆï¼Œå¼ºåˆ¶ç»“æŸå½“å‰episode")
            self.end_episode()
        
        self.dataset = None
        logger.info("æ•°æ®é›†ç®¡ç†å™¨å·²æ¸…ç†")

    def _sync_frame_count(self):
        """åŒæ­¥å¸§è®¡æ•°ä¸å®é™…æ•°æ®é•¿åº¦"""
        try:
            if hasattr(self.dataset, 'episode_buffer'):
                buffer = self.dataset.episode_buffer
                if isinstance(buffer, dict):
                    # åªæ£€æŸ¥å®é™…çš„æ•°æ®å­—æ®µï¼Œå¿½ç•¥å…ƒæ•°æ®å­—æ®µ
                    data_fields = [
                        'observation.images.ee_cam',
                        'observation.images.rgb_rs_0', 
                        'observation.images.rgb_rs_1',
                        'observation.state',
                        'actions',
                        'task',
                        'timestamp',
                        'frame_index'
                    ]
                    
                    lengths = []
                    for key in data_fields:
                        if key in buffer and hasattr(buffer[key], '__len__'):
                            lengths.append(len(buffer[key]))
                    
                    if lengths:
                        # ä½¿ç”¨æœ€å¸¸è§çš„é•¿åº¦ä½œä¸ºå®é™…å¸§æ•°
                        from collections import Counter
                        length_counts = Counter(lengths)
                        actual_frame_count = length_counts.most_common(1)[0][0]
                        
                        if actual_frame_count != self.frame_count:
                            logger.warning(f"åŒæ­¥å¸§è®¡æ•°: {self.frame_count} -> {actual_frame_count}")
                            self.frame_count = actual_frame_count
                            
                            # åŒæ—¶æ›´æ–°bufferçš„sizeå­—æ®µ
                            buffer['size'] = actual_frame_count
                            
                        return actual_frame_count
                    
        except Exception as e:
            logger.warning(f"åŒæ­¥å¸§è®¡æ•°å¤±è´¥: {e}")
            
        return self.frame_count
    
    def _validate_episode_data(self) -> bool:
        """éªŒè¯episodeæ•°æ®çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§"""
        try:
            if not hasattr(self.dataset, 'episode_buffer'):
                logger.error("æ•°æ®é›†æ²¡æœ‰episode_bufferå±æ€§")
                return False
            
            buffer = self.dataset.episode_buffer
            if not isinstance(buffer, dict):
                logger.error(f"episode_bufferç±»å‹é”™è¯¯: {type(buffer)}")
                return False
            
            # æ£€æŸ¥å¿…éœ€çš„å­—æ®µ
            required_keys = [
                'observation.images.ee_cam',
                'observation.images.rgb_rs_0', 
                'observation.images.rgb_rs_1',
                'observation.state',
                'actions'
            ]
            
            missing_keys = []
            for key in required_keys:
                if key not in buffer:
                    missing_keys.append(key)
            
            if missing_keys:
                logger.error(f"ç¼ºå°‘å¿…éœ€çš„æ•°æ®å­—æ®µ: {missing_keys}")
                return False
            
            # æ£€æŸ¥æ•°æ®é•¿åº¦ä¸€è‡´æ€§ - åªæ£€æŸ¥å®é™…çš„æ•°æ®å­—æ®µ
            data_fields = [
                'observation.images.ee_cam',
                'observation.images.rgb_rs_0', 
                'observation.images.rgb_rs_1',
                'observation.state',
                'actions',
                'task',
                'timestamp',
                'frame_index'
            ]
            
            lengths = {}
            for key in data_fields:
                if key in buffer and hasattr(buffer[key], '__len__'):
                    lengths[key] = len(buffer[key])
            
            if not lengths:
                logger.error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®")
                return False
            
            # æ£€æŸ¥æ‰€æœ‰é•¿åº¦æ˜¯å¦ä¸€è‡´
            unique_lengths = set(lengths.values())
            if len(unique_lengths) > 1:
                logger.error(f"æ•°æ®é•¿åº¦ä¸ä¸€è‡´: {dict(lengths)}")
                return False
            
            actual_length = list(unique_lengths)[0]
            
            # æ£€æŸ¥sizeå­—æ®µ
            if 'size' not in buffer:
                logger.warning("ç¼ºå°‘sizeå­—æ®µï¼Œå°†è‡ªåŠ¨è®¾ç½®")
                buffer['size'] = actual_length
            elif buffer['size'] != actual_length:
                logger.warning(f"sizeå­—æ®µä¸åŒ¹é…ï¼Œæ›´æ­£: {buffer['size']} -> {actual_length}")
                buffer['size'] = actual_length
            
            # æ£€æŸ¥æ•°æ®ä¸ä¸ºç©º
            if actual_length == 0:
                logger.error("æ•°æ®é•¿åº¦ä¸º0")
                return False
            
            # æ£€æŸ¥å…³é”®æ•°æ®å­—æ®µçš„æ•°æ®ç±»å‹å’Œå½¢çŠ¶
            for key in required_keys:
                value = buffer[key]
                if not isinstance(value, list) or len(value) == 0:
                    logger.error(f"å­—æ®µ {key} æ•°æ®æ ¼å¼é”™è¯¯")
                    return False
                
                # æ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç±»å‹
                first_elem = value[0]
                if key.startswith('observation.images.'):
                    if not isinstance(first_elem, np.ndarray) or first_elem.shape != (3, 480, 640):
                        logger.error(f"å›¾åƒå­—æ®µ {key} æ•°æ®æ ¼å¼é”™è¯¯: shape={getattr(first_elem, 'shape', 'N/A')}")
                        return False
                elif key == 'observation.state':
                    if not isinstance(first_elem, np.ndarray) or len(first_elem) != 8:
                        logger.error(f"çŠ¶æ€å­—æ®µ {key} æ•°æ®æ ¼å¼é”™è¯¯: shape={getattr(first_elem, 'shape', 'N/A')}")
                        return False
                elif key == 'actions':
                    if not isinstance(first_elem, np.ndarray) or len(first_elem) != 8:
                        logger.error(f"åŠ¨ä½œå­—æ®µ {key} æ•°æ®æ ¼å¼é”™è¯¯: shape={getattr(first_elem, 'shape', 'N/A')}")
                        return False
            
            logger.info(f"âœ… æ•°æ®éªŒè¯é€šè¿‡ï¼Œå…± {actual_length} å¸§æ•°æ®")
            return True
            
        except Exception as e:
            logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False


def test_lerobot_dataset_manager():
    """æµ‹è¯•LeRobotæ•°æ®é›†ç®¡ç†å™¨"""
    logger.info("å¼€å§‹æµ‹è¯•LeRobotæ•°æ®é›†ç®¡ç†å™¨")
    
    # åˆ›å»ºæµ‹è¯•ç›®å½•
    test_dir = "test_dataset"
    manager = LeRobotDatasetManager(dataset_dir=test_dir)
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        if not manager.create_dataset():
            logger.error("åˆ›å»ºæ•°æ®é›†å¤±è´¥")
            return
        
        # å¼€å§‹å½•åˆ¶episode
        if not manager.start_episode():
            logger.error("å¼€å§‹episodeå¤±è´¥")
            return
        
        # æ·»åŠ å‡ å¸§æµ‹è¯•æ•°æ®
        for i in range(5):
            # æ¨¡æ‹Ÿç›¸æœºå›¾åƒï¼ˆCHWæ ¼å¼ï¼‰
            camera_images = {
                "observation.images.ee_cam": np.random.randint(0, 255, (3, 480, 640), dtype=np.uint8),
                "observation.images.rgb_rs_0": np.random.randint(0, 255, (3, 480, 640), dtype=np.uint8),
                "observation.images.rgb_rs_1": np.random.randint(0, 255, (3, 480, 640), dtype=np.uint8)
            }
            
            # æ¨¡æ‹Ÿæœºæ¢°è‡‚çŠ¶æ€ï¼ˆ8ç»´ï¼‰
            robot_state = np.random.rand(8).astype(np.float32)
            
            # æ¨¡æ‹ŸåŠ¨ä½œï¼ˆ8ç»´ï¼‰
            actions = np.random.rand(8).astype(np.float32)
            
            # æ·»åŠ å¸§
            if manager.add_frame(camera_images, robot_state, actions, "test manipulation"):
                logger.info(f"æ·»åŠ å¸§ {i+1} æˆåŠŸ")
            else:
                logger.error(f"æ·»åŠ å¸§ {i+1} å¤±è´¥")
        
        # ç»“æŸepisode
        if manager.end_episode():
            logger.info("Episodeç»“æŸæˆåŠŸ")
        else:
            logger.error("Episodeç»“æŸå¤±è´¥")
        
        # è·å–æ•°æ®é›†ä¿¡æ¯
        info = manager.get_dataset_info()
        logger.info(f"æ•°æ®é›†ä¿¡æ¯: {info}")
        
        logger.info("âœ… æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    finally:
        # æ¸…ç†
        manager.cleanup()
        
        # åˆ é™¤æµ‹è¯•ç›®å½•
        if Path(test_dir).exists():
            shutil.rmtree(test_dir)
            logger.info("æ¸…ç†æµ‹è¯•æ•°æ®")


if __name__ == "__main__":
    test_lerobot_dataset_manager()